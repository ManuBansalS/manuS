{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManuBansalS/manuS/blob/main/ResearchPaper/jupyter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Core libraries\n",
        "import os, random, re, string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# NLP\n",
        "import nltk, spacy\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_fscore_support\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Transformers\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "\n",
        "# Visualization\n",
        "import seaborn as sns\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)"
      ],
      "metadata": {
        "id": "qZbX5j-zbdIE"
      },
      "id": "qZbX5j-zbdIE",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "# Optional: Still set environment variables for Kaggle if needed elsewhere\n",
        "os.environ['KAGGLE_USERNAME'] = \"manubansalg\"\n",
        "os.environ['KAGGLE_KEY'] = \"aa9b0c66c740f641bd7d2a35cdd58660\"\n",
        "\n",
        "# Dataset reference (as in Code 2)\n",
        "dataset_name = \"thoughtvector/customer-support-on-twitter\"\n",
        "\n",
        "# Specify the internal file name—typically the CSV inside the dataset.\n",
        "# The Kaggle dataset likely contains a CSV (often named something like 'customer_support_on_twitter.csv')\n",
        "file_path = \"twcs/twcs.csv\"\n",
        "\n",
        "# Load dataset directly into pandas DataFrame\n",
        "df = kagglehub.load_dataset(\n",
        "    KaggleDatasetAdapter.PANDAS,\n",
        "    dataset_name,\n",
        "    file_path\n",
        ")\n",
        "\n",
        "# Select only desired columns and drop missing values\n",
        "df = df[['text', 'author_id']]  # Adjust as needed—e.g., if 'label' exists instead of 'author_id'\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGDkX775c8-Y",
        "outputId": "1abdaf18-0a38-4a4f-94e7-c6cf963fa1b3"
      },
      "id": "wGDkX775c8-Y",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2041554325.py:18: DeprecationWarning: load_dataset is deprecated and will be removed in a future version.\n",
            "  df = kagglehub.load_dataset(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/thoughtvector/customer-support-on-twitter?dataset_version_number=10&file_name=twcs/twcs.csv...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:05<00:00, 32.5MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting zip of twcs.csv...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text   author_id\n",
            "0  @115712 I understand. I would like to assist y...  sprintcare\n",
            "1      @sprintcare and how do you propose we do that      115712\n",
            "2  @sprintcare I have sent several private messag...      115712\n",
            "3  @115712 Please send us a Private Message so th...  sprintcare\n",
            "4                                 @sprintcare I did.      115712\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "def fast_sentiment(text):\n",
        "    score = sia.polarity_scores(str(text))['compound']\n",
        "    if score >= 0.05:\n",
        "        return \"positive\"\n",
        "    elif score <= -0.05:\n",
        "        return \"negative\"\n",
        "    else:\n",
        "        return \"neutral\"\n",
        "\n",
        "df['sentiment'] = df['text'].apply(fast_sentiment)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Y65rJnydWFo",
        "outputId": "e8f5164d-d690-4821-f512-bf7519822f82"
      },
      "id": "8Y65rJnydWFo",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"http\\S+\", \"\", str(text))  # remove URLs\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = \" \".join([word for word in text.split() if word not in STOPWORDS])\n",
        "    return text\n",
        "\n",
        "df['clean_text'] = df['text'].astype(str).apply(clean_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KvEuzt6i-4a",
        "outputId": "93907394-0779-4c53-a1ed-40ea2072f819"
      },
      "id": "5KvEuzt6i-4a",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# First split into train (80%) and temp (20%)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    df['clean_text'],\n",
        "    df['sentiment'],\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df['sentiment']\n",
        ")\n",
        "\n",
        "# Then split the temp (20%) into validation (10%) and test (10%)\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp,\n",
        "    y_temp,\n",
        "    test_size=0.5,   # half of 20% → 10%\n",
        "    random_state=42,\n",
        "    stratify=y_temp\n",
        ")\n"
      ],
      "metadata": {
        "id": "a7pgd0Cpj7Vg"
      },
      "id": "a7pgd0Cpj7Vg",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# TF-IDF\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_val_tfidf   = tfidf.transform(X_val)\n",
        "X_test_tfidf  = tfidf.transform(X_test)\n",
        "\n",
        "# Logistic Regression\n",
        "logreg = LogisticRegression(max_iter=1000, random_state=SEED)\n",
        "\n",
        "# 5-fold CV on train set\n",
        "cv_scores = cross_val_score(logreg, X_train_tfidf, y_train, cv=5, scoring='f1_macro')\n",
        "print(\"LogReg CV F1 Scores:\", cv_scores, \"Mean:\", cv_scores.mean())\n",
        "\n",
        "# Train & evaluate\n",
        "logreg.fit(X_train_tfidf, y_train)\n",
        "y_pred = logreg.predict(X_test_tfidf)\n",
        "print(\"LogReg Test Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWuKUSvBlO3Q",
        "outputId": "1ad2b923-dd07-4395-f9ff-d68819ae2543"
      },
      "id": "LWuKUSvBlO3Q",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogReg CV F1 Scores: [0.87618983 0.87618299 0.87620369 0.87583312 0.87619163] Mean: 0.8761202512733106\n",
            "LogReg Test Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.85      0.77      0.81     66002\n",
            "     neutral       0.86      0.92      0.89     72447\n",
            "    positive       0.92      0.93      0.93    142729\n",
            "\n",
            "    accuracy                           0.89    281178\n",
            "   macro avg       0.88      0.87      0.88    281178\n",
            "weighted avg       0.89      0.89      0.89    281178\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear SVM\n",
        "svm = LinearSVC(random_state=SEED)\n",
        "\n",
        "# 5-fold CV\n",
        "cv_scores = cross_val_score(svm, X_train_tfidf, y_train, cv=5, scoring='f1_macro')\n",
        "print(\"SVM CV F1 Scores:\", cv_scores, \"Mean:\", cv_scores.mean())\n",
        "\n",
        "# Train & evaluate\n",
        "svm.fit(X_train_tfidf, y_train)\n",
        "y_pred = svm.predict(X_test_tfidf)\n",
        "print(\"SVM Test Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SPZ0G40sE4b",
        "outputId": "cfdb21e1-2e3a-4bb3-804b-ad4daee54669"
      },
      "id": "0SPZ0G40sE4b",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM CV F1 Scores: [0.8770334  0.87730254 0.87691305 0.87671581 0.8771216 ] Mean: 0.8770172820094512\n",
            "SVM Test Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.86      0.77      0.81     66002\n",
            "     neutral       0.86      0.92      0.89     72447\n",
            "    positive       0.92      0.94      0.93    142729\n",
            "\n",
            "    accuracy                           0.89    281178\n",
            "   macro avg       0.88      0.87      0.88    281178\n",
            "weighted avg       0.89      0.89      0.89    281178\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3XoQ1d3zTwl",
        "outputId": "919a1665-9606-410d-d7ce-1a98c2b530fc"
      },
      "id": "l3XoQ1d3zTwl",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, roc_auc_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ✅ Device check\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "torch.backends.cudnn.benchmark = True   # speedup on GPU\n",
        "\n",
        "# ✅ Use Twitter RoBERTa (3-class sentiment: negative, neutral, positive)\n",
        "model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "model.half()   # FP16 for speed/memory\n",
        "\n",
        "# ✅ Prediction function\n",
        "@torch.inference_mode()\n",
        "def predict_labels(texts, batch_size=32, max_length=128):\n",
        "    preds, confs, probs_all = [], [], []\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"RoBERTa inference\"):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        enc = tokenizer(\n",
        "            batch,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(device)\n",
        "\n",
        "        outputs = model(**enc)\n",
        "        probs = torch.softmax(outputs.logits, dim=-1)\n",
        "        conf, ids = torch.max(probs, dim=-1)\n",
        "        preds.extend(ids.cpu().numpy())\n",
        "        confs.extend(conf.cpu().numpy())\n",
        "        probs_all.extend(probs.cpu().numpy())\n",
        "    return preds, confs, np.array(probs_all)\n",
        "\n",
        "# ✅ Run on Validation and Test\n",
        "val_texts = list(X_val)\n",
        "test_texts = list(X_test)\n",
        "\n",
        "val_preds, _, val_probs = predict_labels(val_texts, batch_size=32)\n",
        "test_preds, _, test_probs = predict_labels(test_texts, batch_size=32)\n",
        "\n",
        "# ✅ Map string labels to numeric\n",
        "label2id = {\"negative\":0, \"neutral\":1, \"positive\":2}\n",
        "y_val_mapped = [label2id[str(l).lower()] for l in y_val]\n",
        "y_test_mapped = [label2id[str(l).lower()] for l in y_test]\n",
        "\n",
        "# ✅ Metrics\n",
        "print(\"\\nValidation Evaluation:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_val_mapped, val_preds))\n",
        "print(\"Macro-F1:\", f1_score(y_val_mapped, val_preds, average=\"macro\"))\n",
        "print(\"Classification Report:\\n\", classification_report(y_val_mapped, val_preds, target_names=[\"negative\",\"neutral\",\"positive\"]))\n",
        "\n",
        "print(\"\\nTest Evaluation:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test_mapped, test_preds))\n",
        "print(\"Macro-F1:\", f1_score(y_test_mapped, test_preds, average=\"macro\"))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test_mapped, test_preds, target_names=[\"negative\",\"neutral\",\"positive\"]))\n",
        "\n",
        "# ✅ Confusion Matrix (Test set)\n",
        "cm = confusion_matrix(y_test_mapped, test_preds)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"negative\",\"neutral\",\"positive\"], yticklabels=[\"negative\",\"neutral\",\"positive\"])\n",
        "plt.title(\"Confusion Matrix - Test Set\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.show()\n",
        "\n",
        "# ✅ ROC Curve (One-vs-Rest, Test set)\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "y_test_bin = label_binarize(y_test_mapped, classes=[0,1,2])\n",
        "fpr, tpr, roc_auc = {}, {}, {}\n",
        "\n",
        "for i in range(3):  # 3 classes\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], test_probs[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "plt.figure(figsize=(7,6))\n",
        "for i, color in zip(range(3), [\"red\",\"green\",\"blue\"]):\n",
        "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
        "             label=f\"Class {i} (AUC = {roc_auc[i]:.2f})\")\n",
        "plt.plot([0,1],[0,1],\"k--\", lw=2)\n",
        "plt.xlim([0.0,1.0])\n",
        "plt.ylim([0.0,1.05])\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curves - Test Set\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# ✅ Full dataset inference (optional)\n",
        "all_preds, all_confs, _ = predict_labels(df['clean_text'].tolist(), batch_size=32)\n",
        "df['bert_label'] = all_preds\n",
        "df['bert_conf'] = all_confs\n",
        "df.to_csv(\"bert_sentiment_full.csv\", index=False)\n",
        "print(\"Saved full dataset predictions to bert_sentiment_full.csv\")\n"
      ],
      "metadata": {
        "id": "lLcoi9SkDW__"
      },
      "id": "lLcoi9SkDW__",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}